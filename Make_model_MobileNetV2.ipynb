{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"r2A2OF5MNBCL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5dfab700-5e30-4db2-bcf8-88b9c734c8ac","executionInfo":{"status":"ok","timestamp":1667163900242,"user_tz":-330,"elapsed":2811,"user":{"displayName":"Sumit Soni","userId":"11076994093410361614"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import tensorflow as tf\n","import cv2\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","source":["using the famous FER-2013 as dataset to train and test model"],"metadata":{"id":"rPF00rULNcVq"}},{"cell_type":"code","source":["Datadirectory_train='/content/drive/MyDrive/ML_Project/train'\n","Datadirectory_test='/content/drive/MyDrive/ML_Project/test'\n","# path=os.path.join(Datadirectory,\"\")\n","# print(path)\n","Classes=['happy','sad']"],"metadata":{"id":"YcX6RGSF_25u","executionInfo":{"status":"ok","timestamp":1667163903682,"user_tz":-330,"elapsed":680,"user":{"displayName":"Sumit Soni","userId":"11076994093410361614"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Changing the size of image from 48X48 to 224X224 to easily put it in the model(now doing it only for whole dataset)"],"metadata":{"id":"1hCKeyrtBnRS"}},{"cell_type":"code","source":["# training_data=[]\n","# def create_training_data():\n","#   for Category in Classes:\n","#     path=os.path.join(Datadirectory_train,Category)\n","#     class_num=Classes.index(Category)\n","#     for img in os.listdir(path):\n","#       img_arr=cv2.imread(os.path.join(path,img))\n","#       new_array=cv2.resize(img_arr,(224,224))\n","#       training_data.append([new_array,class_num])\n","\n","# create_training_data()\n"],"metadata":{"id":"U8agNhfNBukC","executionInfo":{"status":"ok","timestamp":1667163924089,"user_tz":-330,"elapsed":18899,"user":{"displayName":"Sumit Soni","userId":"11076994093410361614"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# print((training_data[0][0]))\n","# print((training_data[0][1]))"],"metadata":{"id":"p4sv5N52-Yl4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import random\n","# random.shuffle(training_data)\n","# X=[]\n","# Y=[]\n","# for features,label in training_data:\n","#   X.append(features)\n","#   Y.append(label)\n","# X=np.array(X).reshape(-1,224,224,3) #changing dimensions to properly allow model to work\n","\n"],"metadata":{"id":"P6tkvPPTqRyX","executionInfo":{"status":"ok","timestamp":1667163930439,"user_tz":-330,"elapsed":695,"user":{"displayName":"Sumit Soni","userId":"11076994093410361614"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# X=X/255"],"metadata":{"id":"3HqYmvHItdhx","executionInfo":{"status":"ok","timestamp":1667163932991,"user_tz":-330,"elapsed":1081,"user":{"displayName":"Sumit Soni","userId":"11076994093410361614"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# print(len(X),len(X[0]),len(X[0][0]),(X[0][0][0]))"],"metadata":{"id":"SHjqwEr6MtOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","model=tf.keras.applications.MobileNetV2() #use MobileNetV2 archutecture pre-training model, as it being light weight we can use it for live demo"],"metadata":{"id":"j7CqRa_9EoT4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667163937045,"user_tz":-330,"elapsed":2034,"user":{"displayName":"Sumit Soni","userId":"11076994093410361614"}},"outputId":"7e7a408f-0604-4f6e-8de0-47cbfcbdcbc8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n","14536120/14536120 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"markdown","source":["Doing transfer learning to make the model learn classes from last step, and hence we won't have to train the whole model from start, and we can use the already existing weights and biases"],"metadata":{"id":"9-5tNW-YFUGs"}},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","BATCH_SIZE = 64\n","\n","train_generator = ImageDataGenerator(rotation_range=90, \n","                                     brightness_range=[0.1, 0.7],\n","                                     width_shift_range=0.5, \n","                                     height_shift_range=0.5,\n","                                     horizontal_flip=True, \n","                                     vertical_flip=True,\n","                                     validation_split=0.15,\n","                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n","\n","test_generator = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing"],"metadata":{"id":"eGqs6u9jz6OX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["traingen = train_generator.flow_from_directory(Datadirectory_train,\n","                                               target_size=(150, 150),\n","                                               class_mode='categorical',\n","                                               classes=Classes,\n","                                               subset='training',\n","                                               batch_size=BATCH_SIZE, \n","                                               shuffle=True,\n","                                               seed=42)\n","\n","validgen = train_generator.flow_from_directory(Datadirectory_train,\n","                                               target_size=(150, 150),\n","                                               class_mode='categorical',\n","                                               classes=Classes,\n","                                               subset='validation',\n","                                               batch_size=BATCH_SIZE,\n","                                               shuffle=True,\n","                                               seed=42)\n","\n","testgen = test_generator.flow_from_directory(Datadirectory_test,\n","                                             target_size=(150, 150),\n","                                             class_mode=None,\n","                                             classes=Classes,\n","                                             batch_size=1,\n","                                             shuffle=False,\n","                                             seed=42)"],"metadata":{"id":"ZmKZ2T4dz8sK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.layers import Dense, Dropout, Flatten\n","from keras.models import Model\n","from keras.optimizers import Adam\n","def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n","    \"\"\"\n","    Compiles a model integrated with VGG16 pretrained layers\n","    \n","    input_shape: tuple - the shape of input images (width, height, channels)\n","    n_classes: int - number of classes for the output layer\n","    optimizer: string - instantiated optimizer to use for training. Defaults to 'RMSProp'\n","    fine_tune: int - The number of pre-trained layers to unfreeze.\n","                If set to 0, all pretrained layers will freeze during training\n","    \"\"\"\n","    \n","    # Pretrained convolutional layers are loaded using the Imagenet weights.\n","    # Include_top is set to False, in order to exclude the model's fully-connected layers.\n","    conv_base = MobileNetV2(include_top=False,\n","                     weights='imagenet', \n","                     input_shape=input_shape)\n","    \n","\n","    top_model = conv_base.output\n","    top_model = Flatten(name=\"flatten\")(top_model)\n","    output_layer = Dense(n_classes, activation='softmax')(top_model)\n","    \n","    # Group the convolutional base and new fully-connected layers into a Model object.\n","    model = Model(inputs=conv_base.input, outputs=output_layer)\n","\n","    # Compiles the model for training.\n","    model.compile(optimizer=optimizer, \n","                  loss='categorical_crossentropy',\n","                  metrics=['accuracy'])\n","    \n","    return model"],"metadata":{"id":"Odsf1b8jz_iX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_shape = (150, 150, 3)\n","optim_1 = Adam(learning_rate=0.001)\n","n_classes=2\n","\n","n_steps = traingen.samples // BATCH_SIZE\n","n_val_steps = validgen.samples // BATCH_SIZE\n","n_epochs = 50\n","\n","# First we'll train the model without Fine-tuning\n","MobileNet_V2_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)"],"metadata":{"id":"Jnv9m56oGlDk","executionInfo":{"status":"ok","timestamp":1667163939724,"user_tz":-330,"elapsed":453,"user":{"displayName":"Sumit Soni","userId":"11076994093410361614"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["!pip install livelossplot\n","from livelossplot.inputs.keras import PlotLossesCallback\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","plot_loss_1 = PlotLossesCallback()\n","\n","# ModelCheckpoint callback - save best weights\n","tl_checkpoint_1 = ModelCheckpoint(filepath='tl_model_v1.weights.best.hdf5',\n","                                  save_best_only=True,\n","                                  verbose=1)\n","\n","# EarlyStopping\n","early_stop = EarlyStopping(monitor='val_loss',\n","                           patience=10,\n","                           restore_best_weights=True,\n","                           mode='min')"],"metadata":{"id":"1B2uIfgs3CVf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Mobilenet_history = MobileNet_v2_model.fit(traingen,\n","                            batch_size=BATCH_SIZE,\n","                            epochs=n_epochs,\n","                            validation_data=validgen,\n","                            steps_per_epoch=n_steps,\n","                            validation_steps=n_val_steps,\n","                            callbacks=[tl_checkpoint_1, early_stop, plot_loss_1],\n","                            verbose=1)"],"metadata":{"id":"EZOsweBC0b0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install livelossplot\n","from livelossplot.inputs.keras import PlotLossesCallback\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","plot_loss_1 = PlotLossesCallback()\n","\n","# ModelCheckpoint callback - save best weights\n","tl_checkpoint_1 = ModelCheckpoint(filepath='tl_model_v1.weights.best.hdf5',\n","                                  save_best_only=True,\n","                                  verbose=1)\n","\n","# EarlyStopping\n","early_stop = EarlyStopping(monitor='val_loss',\n","                           patience=10,\n","                           restore_best_weights=True,\n","                           mode='min')"],"metadata":{"id":"-nK-wIgPZKQh","executionInfo":{"status":"ok","timestamp":1667163942981,"user_tz":-330,"elapsed":1478,"user":{"displayName":"Sumit Soni","userId":"11076994093410361614"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["MobileNet_V2_model.load_weights('tl_model_v1.weights.best.hdf5') # initialize the best trained weights\n","\n","MobileNet_preds_ft = MobileNet_V2_model.predict(testgen)\n","MobileNet_pred_classes_ft = np.argmax(MobileNet_preds_ft, axis=1)"],"metadata":{"id":"CXHI9jvx0ouF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MobileNet_acc_ft = accuracy_score(true_classes, MobieNet_pred_classes_ft)\n","print(\"VGG16 Model Accuracy with Fine-Tuning: {:.2f}%\".format(MobileNet_acc_ft * 100))"],"metadata":{"id":"-byCsaosO0cV"},"execution_count":null,"outputs":[]}]}